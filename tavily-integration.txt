
https://community.vercel.com/t/how-do-i-connect-tavily-mcp-with-vercel-ai-sdk/20947
How do I connect Tavily MCP with Vercel AI SDK?
AI SDK

Mxhiraz
30d
How to connect Tavily (https://www.tavily.com) MCP with the Vercel AI SDK



56
views

4
links



Unlisted on Aug 28


Listed on Aug 28


Amy Egan 
Vercel Staff
30d
Asked v0 for the answer, and here’s what it gave me:

You can connect Tavily’s MCP (Model Context Protocol) server with the Vercel AI SDK using the new MCP client support introduced in AI SDK 4.2.

Here’s how to set it up:

Using Tavily’s Remote MCP Server
The easiest way is to use Tavily’s remote MCP server directly:

import { createMCPClient } from '@ai-sdk/mcp'

const mcpClient = createMCPClient({
  name: 'tavily',
  version: '1.0.0',
  transport: {
    type: 'http',
    url: 'https://mcp.tavily.com/mcp/?tavilyApiKey=YOUR_TAVILY_API_KEY'
  }
})
Using with AI SDK Functions
Once connected, you can use Tavily’s search tools in your AI SDK functions:

import { generateText } from 'ai'
import { openai } from '@ai-sdk/openai'

const { text } = await generateText({
  model: openai('gpt-4'),
  prompt: 'Search for the latest news about AI developments',
  tools: await mcpClient.getTools(), // This includes tavily-search, tavily-extract, etc.
})
Available Tavily Tools
The Tavily MCP server provides several tools:

tavily-search: Real-time web search
tavily-extract: Intelligent data extraction from web pages
tavily-map: Creates structured maps of websites
tavily-crawl: Systematically explores websites
Make sure to get your Tavily API key from tavily.com and replace YOUR_TAVILY_API_KEY in the URL.

Resources:

AI SDK MCP Documentation
Tavily MCP GitHub Repository
I hope that helps!

_________________________________________________________________________________________________________________________________________________________
https://ai-sdk.dev/docs/reference/ai-sdk-core/create-mcp-client
AI SDK Core
experimental_createMCPClient
experimental_createMCPClient()
Creates a lightweight Model Context Protocol (MCP) client that connects to an MCP server. The client's primary purpose is tool conversion between MCP tools and AI SDK tools.

It currently does not support accepting notifications from an MCP server, and custom configuration of the client.

This feature is experimental and may change or be removed in the future.

Import
import { experimental_createMCPClient } from "ai"
API Signature
Parameters
config:
MCPClientConfig
Configuration for the MCP client.
MCPClientConfig
transport:
TransportConfig = MCPTransport | McpSSEServerConfig
Configuration for the message transport layer.
MCPTransport
start:
() => Promise<void>
A method that starts the transport
send:
(message: JSONRPCMessage) => Promise<void>
A method that sends a message through the transport
close:
() => Promise<void>
A method that closes the transport
onclose:
() => void
A method that is called when the transport is closed
onerror:
(error: Error) => void
A method that is called when the transport encounters an error
onmessage:
(message: JSONRPCMessage) => void
A method that is called when the transport receives a message
McpSSEServerConfig
type:
'sse'
Use Server-Sent Events for communication
url:
string
URL of the MCP server
headers?:
Record<string, string>
Additional HTTP headers to be sent with requests.
name?:
string
Client name. Defaults to "ai-sdk-mcp-client"
onUncaughtError?:
(error: unknown) => void
Handler for uncaught errors
Returns
Returns a Promise that resolves to an MCPClient with the following methods:

tools:
async (options?: { schemas?: TOOL_SCHEMAS }) => Promise<McpToolSet<TOOL_SCHEMAS>>
Gets the tools available from the MCP server.
options
schemas?:
TOOL_SCHEMAS
Schema definitions for compile-time type checking. When not provided, schemas are inferred from the server.
close:
async () => void
Closes the connection to the MCP server and cleans up resources.
Example

import { experimental_createMCPClient, generateText } from 'ai';
import { Experimental_StdioMCPTransport } from 'ai/mcp-stdio';
import { openai } from '@ai-sdk/openai';

let client;

try {
  client = await experimental_createMCPClient({
    transport: new Experimental_StdioMCPTransport({
      command: 'node server.js',
    }),
  });

  const tools = await client.tools();

  const response = await generateText({
    model: openai('gpt-4o-mini'),
    tools,
    messages: [{ role: 'user', content: 'Query the data' }],
  });

  console.log(response);
} catch (error) {
  console.error('Error:', error);
} finally {
  // ensure the client is closed even if an error occurs
  if (client) {
    await client.close();
  }
}
Error Handling
The client throws MCPClientError for:

Client initialization failures
Protocol version mismatches
Missing server capabilities
Connection failures
For tool execution, errors are propagated as CallToolError errors.

For unknown errors, the client exposes an onUncaughtError callback that can be used to manually log or handle errors that are not covered by known error types.
_________________________________________________________________________________________________________________________________________________________
https://ai-sdk.dev/docs/ai-sdk-core/mcp-tools
AI SDK Core
Model Context Protocol (MCP) Tools
Model Context Protocol (MCP) Tools
The MCP tools feature is experimental and may change in the future.

The AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools. This enables your AI applications to discover and use tools across various services through a standardized interface.

Initializing an MCP Client
We recommend using HTTP transport (like StreamableHTTPClientTransport) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.

Create an MCP client using one of the following transport options:

HTTP transport (Recommended): Use transports from MCP's official TypeScript SDK like StreamableHTTPClientTransport for production deployments
SSE (Server-Sent Events): An alternative HTTP-based transport
stdio: For local development only. Uses standard input/output streams for local MCP servers
HTTP Transport (Recommended)
For production deployments, we recommend using HTTP transports like StreamableHTTPClientTransport from MCP's official TypeScript SDK:


import { experimental_createMCPClient as createMCPClient } from 'ai';
import { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/client/streamableHttp.js';

const url = new URL('https://your-server.com/mcp');
const mcpClient = await createMCPClient({
  transport: new StreamableHTTPClientTransport(url, {
    sessionId: 'session_123',
  }),
});
SSE Transport
SSE provides an alternative HTTP-based transport option. Configure it with a type and url property:


import { experimental_createMCPClient as createMCPClient } from 'ai';

const mcpClient = await createMCPClient({
  transport: {
    type: 'sse',
    url: 'https://my-server.com/sse',

    // optional: configure HTTP headers, e.g. for authentication
    headers: {
      Authorization: 'Bearer my-api-key',
    },
  },
});
Stdio Transport (Local Servers)
The stdio transport should only be used for local servers.

The Stdio transport can be imported from either the MCP SDK or the AI SDK:


import { experimental_createMCPClient as createMCPClient } from 'ai';
import { StdioClientTransport } from '@modelcontextprotocol/sdk/client/stdio.js';
// Or use the AI SDK's stdio transport:
// import { Experimental_StdioMCPTransport as StdioClientTransport } from 'ai/mcp-stdio';

const mcpClient = await createMCPClient({
  transport: new StdioClientTransport({
    command: 'node',
    args: ['src/stdio/dist/server.js'],
  }),
});
Custom Transport
You can also bring your own transport by implementing the MCPTransport interface for specific requirements not covered by the standard transports.

The client returned by the experimental_createMCPClient function is a lightweight client intended for use in tool conversion. It currently does not support all features of the full MCP client, such as: authorization, session management, resumable streams, and receiving notifications.

Closing the MCP Client
After initialization, you should close the MCP client based on your usage pattern:

For short-lived usage (e.g., single requests), close the client when the response is finished
For long-running clients (e.g., command line apps), keep the client open but ensure it's closed when the application terminates
When streaming responses, you can close the client when the LLM response has finished. For example, when using streamText, you should use the onFinish callback:


const mcpClient = await experimental_createMCPClient({
  // ...
});

const tools = await mcpClient.tools();

const result = await streamText({
  model: 'openai/gpt-4.1',
  tools,
  prompt: 'What is the weather in Brooklyn, New York?',
  onFinish: async () => {
    await mcpClient.close();
  },
});
When generating responses without streaming, you can use try/finally or cleanup functions in your framework:


let mcpClient: MCPClient | undefined;

try {
  mcpClient = await experimental_createMCPClient({
    // ...
  });
} finally {
  await mcpClient?.close();
}
Using MCP Tools
The client's tools method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:

Schema Discovery
With schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:


const tools = await mcpClient.tools();
This approach is simpler to implement and automatically stays in sync with server changes. However, you won't have TypeScript type safety during development, and all tools from the server will be loaded

Schema Definition
For better type safety and control, you can define the tools and their input schemas explicitly in your client code:


import { z } from 'zod';

const tools = await mcpClient.tools({
  schemas: {
    'get-data': {
      inputSchema: z.object({
        query: z.string().describe('The data query'),
        format: z.enum(['json', 'text']).optional(),
      }),
    },
    // For tools with zero inputs, you should use an empty object:
    'tool-with-no-args': {
      inputSchema: z.object({}),
    },
  },
});
This approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define schemas, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs

_________________________________________________________________________________________________________________________________________________________
